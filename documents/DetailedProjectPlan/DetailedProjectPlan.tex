\documentclass{article}
\usepackage[style=numeric]{biblatex}
\usepackage{multirow}
\addbibresource{references.bib}
 
\title{FYP 2019-20 Detailed Project Plan \\ Building new AI applications for a distributed AI serving system}
\author{Waqas Ali\\{\small Supervisor: Dr. Heming Cui}\\{\small Mentor: Shixiong Zhao}}
\date{\today}
 
\begin{document}
\maketitle
 
\section{Background}
Artificial intelligence is an area of computer science which focuses on granting machines the ability to act intelligently \cite{McCarthy2007}. It's a vast field with limitless applications and each application has its own unique solution. Machine learning, specifically, is a subset of artificial intelligence which learns from data. \cite{Mitchell1997} Nowadays we see artificial intelligence everywhere from spam filters \cite{Androutsopoulos2000} to movie recommendations \cite{lekakos2008hybrid} to virtual assistants to self-driving.

Customers are demanding smarter and smarter capabilities in their machines which leads to its own set of software development challenges; Nvidia summarises them with the PLASTER \cite{Teich2018} framework :
\begin{itemize}
  \item Programmability
  \item Latency
  \item Accuracy
  \item Size of Model
  \item Throughput
  \item Energy Efficiency
  \item Rate of Learning
\end{itemize}

The development lifecycle of any machine learning application can be summarized as follows:
\begin{enumerate}
  \item Training (Learning from data)
  \item Inference (Returning an output given a single input)
\end{enumerate}

End-users of machine learning applications are only concerned with inference. For example, for virtual assistants training may take several days going through tonnes of voice recordings and figuring out what sounds correspond to which words. However, the customer is only concerned with sending their own voice instruction and expecting the assistant to understand their instruction. In this scenario, the latter is inference and real-time response (low latency) is expected. Moreover, the virtual assistant should be able to cope up with large traffic as well (high throughput).

Therefore, as also mentioned in the PLASTER framework \cite{Teich2018}, latency and throughput are highly important for any artificial intelligence application.

It's typical of such applications to have pipelines made out of several compute-intensive tasks. Some of them could even be run in parallel. To make such applications faster, it makes sense to distribute these tasks across several machines and that is exactly where distributed architecture could shine.

\section{Objective}
The project's aim is to improve latency and throughput of AI applications by implementing and deploying them on a distributed system. As a proof of concept, an AI application will be chosen which has a complex inference pipeline and it will be developed on a distributed system.

To prove that a distributed architecture can indeed improve latency and throughput, performance will be compared across systems of various specifications:
\begin{enumerate}
  \item 1 node for n tasks (baseline)
  \item less than n nodes for n tasks
  \item n nodes for n tasks (optimum)
\end{enumerate}

\section{Methodology}

\begin{enumerate}
  \item Choose AI Application i.e. Smart Driving, Stock Price Prediction, etc.
  \item Develop and run basic inference pipeline on a monolith architecture.
  \item Convert the pipeline to run using RPC (remote procedure call) on one server.
  \item Deploy the above on multiple servers.
  \item Devise a method to programmatically instantiate cloud resources and deploy model.
  \item Compare latency and throughput of model on distributed systems of various specifications.
\end{enumerate}

\section{Schedule \& Milestones}
\renewcommand{\arraystretch}{2}
\begin{tabular}{ |l|l| } 
\hline
\multicolumn{2}{|c|}{2019} \\ \hline
\multirow{2}{*}{October} & Choose \& Design AI application to work on \\
 & Develop a basic inference pipeline on a monolithic architecture \\ \hline
November & Convert pipeline to work on a distributed system using RPC \\ \hline
December & Manually deploy pipeline to distributed architecture on cloud \\ \hline
\multicolumn{2}{|c|}{2020} \\ \hline
\multirow{3}{*}{January} & Programmatically instantiate cloud resources and deploy model \\
 & First Presentation \\
 & Detailed interim report \\ \hline
February     & Vary deployments by cloud resources and measure latency/throughput on each  \\ \hline
March     & Enhance AI inference pipeline by adding more steps  \\ \hline
\multirow{3}{*}{April} & Finalize implementation \\
 & Final Presentation \\
 & Final Report \\ \hline
\end{tabular}
\renewcommand{\arraystretch}{1}

\printbibliography

\end{document}